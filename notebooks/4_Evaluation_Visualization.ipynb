{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Captioning - Evaluation and Visualization\n",
    "\n",
    "In this notebook, we will evaluate our trained image captioning model on the test set and visualize the results. We will:\n",
    "\n",
    "1. Load the trained model and vocabulary\n",
    "2. Generate captions for test images\n",
    "3. Compute evaluation metrics (BLEU-1 to BLEU-4)\n",
    "4. Analyze model performance across different image categories\n",
    "5. Visualize attention weights (if implemented)\n",
    "6. Create an interactive demo for inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import pickle\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "import nltk\n",
    "from nltk.translate.bleu_score import corpus_bleu, sentence_bleu, SmoothingFunction\n",
    "\n",
    "# Add project root to path\n",
    "sys.path.append('..')\n",
    "\n",
    "# Import project modules\n",
    "from models.caption_model import CaptionModel\n",
    "from utils.vocabulary import Vocabulary\n",
    "from utils.dataset import get_data_loaders\n",
    "from utils.metrics import calculate_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Model and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define paths\n",
    "model_dir = '../models'\n",
    "config_path = os.path.join(model_dir, 'config.json')\n",
    "model_path = os.path.join(model_dir, 'best_model_loss.pth')  # or 'best_model_bleu.pth'\n",
    "vocab_path = os.path.join(model_dir, 'vocabulary.pkl')\n",
    "data_dir = '../data/flickr8k'\n",
    "\n",
    "# Load configuration\n",
    "with open(config_path, 'r') as f:\n",
    "    config = json.load(f)\n",
    "\n",
    "# Print configuration\n",
    "print(\"Model configuration:\")\n",
    "for key, value in config.items():\n",
    "    print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load vocabulary\n",
    "with open(vocab_path, 'rb') as f:\n",
    "    vocab = pickle.load(f)\n",
    "\n",
    "print(f\"Loaded vocabulary with {len(vocab)} words\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Create model\n",
    "model = CaptionModel(\n",
    "    embed_size=int(config['embed_size']),\n",
    "    hidden_size=int(config['hidden_size']),\n",
    "    vocab_size=len(vocab),\n",
    "    num_layers=int(config['num_layers']),\n",
    "    encoder_model=config['encoder_model'],\n",
    "    decoder_type=config['decoder_type'],\n",
    "    dropout=float(config['dropout'])\n",
    ")\n",
    "\n",
    "# Load model weights\n",
    "checkpoint = torch.load(model_path, map_location=device)\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "model = model.to(device)\n",
    "model.eval()\n",
    "\n",
    "print(f\"Model loaded from {model_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get data loaders\n",
    "_, _, test_loader, _ = get_data_loaders(\n",
    "    data_dir=data_dir,\n",
    "    batch_size=1,  # Process one image at a time for evaluation\n",
    "    num_workers=4\n",
    ")\n",
    "\n",
    "print(f\"Loaded test set with {len(test_loader)} images\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Generate Captions for Test Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate captions for all test images\n",
    "def generate_test_captions(model, test_loader, vocab, device, beam_size=1):\n",
    "    \"\"\"Generate captions for all test images.\"\"\"\n",
    "    results = ...\n",
    "    # TODO: Implement caption generation for test images\n",
    "    # 1. Initialize a list to store results\n",
    "    # 2. Set model to evaluation mode\n",
    "    # 3. For each batch in the test loader:\n",
    "    #    a. Move images to the appropriate device\n",
    "    #    b. Generate captions using the model\n",
    "    #    c. Decode the predicted caption tokens to text\n",
    "    #    d. Decode the ground truth caption to text\n",
    "    #    e. Store results (image_id, ground truth, prediction)\n",
    "    # 4. Return results as a DataFrame\n",
    "    \n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "# Generate captions with greedy search\n",
    "greedy_results = generate_test_captions(model, test_loader, vocab, device, beam_size=1)\n",
    "print(f\"Generated captions for {len(greedy_results)} test images\")\n",
    "\n",
    "# Show a few examples\n",
    "greedy_results.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate captions with beam search (this might take longer)\n",
    "beam_results = generate_test_captions(model, test_loader, vocab, device, beam_size=3)\n",
    "print(f\"Generated captions with beam search for {len(beam_results)} test images\")\n",
    "\n",
    "# Compare to greedy search\n",
    "comparison_df = pd.DataFrame({\n",
    "    'image_id': greedy_results['image_id'],\n",
    "    'ground_truth': greedy_results['ground_truth'],\n",
    "    'greedy_search': greedy_results['predicted'],\n",
    "    'beam_search': beam_results['predicted']\n",
    "})\n",
    "\n",
    "# Show where they differ\n",
    "differ_mask = comparison_df['greedy_search'] != comparison_df['beam_search']\n",
    "print(f\"Beam search produced different captions for {differ_mask.sum()} images ({differ_mask.sum() / len(comparison_df) * 100:.1f}%)\")\n",
    "\n",
    "# Show a few examples where they differ\n",
    "comparison_df[differ_mask].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Calculate Evaluation Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate BLEU scores\n",
    "def calculate_bleu_scores(results_df):\n",
    "    \"\"\"Calculate BLEU-1 to BLEU-4 scores for generated captions.\"\"\"\n",
    "\n",
    "    bleu_scores = ...\n",
    "    # TODO: Implement BLEU score calculation\n",
    "    # 1. Initialize lists for references and hypotheses\n",
    "    # 2. For each row in results_df:\n",
    "    #    a. Tokenize ground truth and predicted captions\n",
    "    #    b. Add tokenized ground truth as a reference (in a list)\n",
    "    #    c. Add tokenized prediction as a hypothesis\n",
    "    # 3. Set up smoothing function for BLEU score calculation\n",
    "    # 4. Calculate BLEU-1 to BLEU-4 scores using corpus_bleu\n",
    "    # 5. Return list of BLEU scores\n",
    "    \n",
    "    return bleu_scores\n",
    "\n",
    "# Calculate BLEU scores for greedy search\n",
    "greedy_bleu = calculate_bleu_scores(greedy_results)\n",
    "print(\"BLEU scores for greedy search:\")\n",
    "for i, score in enumerate(greedy_bleu):\n",
    "    print(f\"  BLEU-{i+1}: {score:.4f}\")\n",
    "\n",
    "# Calculate BLEU scores for beam search\n",
    "beam_bleu = calculate_bleu_scores(beam_results)\n",
    "print(\"\\nBLEU scores for beam search:\")\n",
    "for i, score in enumerate(beam_bleu):\n",
    "    print(f\"  BLEU-{i+1}: {score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare BLEU scores\n",
    "bleu_comparison = pd.DataFrame({\n",
    "    'Metric': ['BLEU-1', 'BLEU-2', 'BLEU-3', 'BLEU-4'],\n",
    "    'Greedy Search': greedy_bleu,\n",
    "    'Beam Search': beam_bleu\n",
    "})\n",
    "\n",
    "# Plot comparison\n",
    "plt.figure(figsize=(10, 6))\n",
    "bar_width = 0.35\n",
    "x = np.arange(len(bleu_comparison['Metric']))\n",
    "\n",
    "plt.bar(x - bar_width/2, bleu_comparison['Greedy Search'], bar_width, label='Greedy Search')\n",
    "plt.bar(x + bar_width/2, bleu_comparison['Beam Search'], bar_width, label='Beam Search')\n",
    "\n",
    "plt.xlabel('Metric')\n",
    "plt.ylabel('Score')\n",
    "plt.title('BLEU Scores Comparison')\n",
    "plt.xticks(x, bleu_comparison['Metric'])\n",
    "plt.legend()\n",
    "plt.ylim(0, max(max(greedy_bleu), max(beam_bleu)) * 1.1)  # Add some margin at the top\n",
    "\n",
    "# Add value labels on bars\n",
    "for i, v in enumerate(bleu_comparison['Greedy Search']):\n",
    "    plt.text(i - bar_width/2, v + 0.01, f'{v:.3f}', ha='center')\n",
    "    \n",
    "for i, v in enumerate(bleu_comparison['Beam Search']):\n",
    "    plt.text(i + bar_width/2, v + 0.01, f'{v:.3f}', ha='center')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Analyze Results by Caption Length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add caption lengths to the results dataframe\n",
    "greedy_results['gt_length'] = greedy_results['ground_truth'].apply(lambda x: len(nltk.word_tokenize(x)))\n",
    "greedy_results['pred_length'] = greedy_results['predicted'].apply(lambda x: len(nltk.word_tokenize(x)))\n",
    "\n",
    "# Calculate average lengths\n",
    "avg_gt_length = greedy_results['gt_length'].mean()\n",
    "avg_pred_length = greedy_results['pred_length'].mean()\n",
    "\n",
    "print(f\"Average ground truth caption length: {avg_gt_length:.2f} words\")\n",
    "print(f\"Average predicted caption length: {avg_pred_length:.2f} words\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot caption length distributions\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "sns.histplot(greedy_results['gt_length'], kde=True, bins=15, color='blue')\n",
    "plt.axvline(avg_gt_length, color='red', linestyle='--', label=f'Mean: {avg_gt_length:.2f}')\n",
    "plt.title('Ground Truth Caption Lengths')\n",
    "plt.xlabel('Number of Words')\n",
    "plt.ylabel('Frequency')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "sns.histplot(greedy_results['pred_length'], kde=True, bins=15, color='green')\n",
    "plt.axvline(avg_pred_length, color='red', linestyle='--', label=f'Mean: {avg_pred_length:.2f}')\n",
    "plt.title('Predicted Caption Lengths')\n",
    "plt.xlabel('Number of Words')\n",
    "plt.ylabel('Frequency')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze BLEU scores by caption length\n",
    "def analyze_bleu_by_length(results_df):\n",
    "    \"\"\"Calculate BLEU scores grouped by caption length.\"\"\"\n",
    "    bleu_df = ...\n",
    "    # TODO: Implement BLEU score analysis by caption length\n",
    "    # 1. Create length bins for categorizing captions\n",
    "    # 2. Assign each caption to a length bin\n",
    "    # 3. For each bin:\n",
    "    #    a. Filter results to include only captions in that bin\n",
    "    #    b. Calculate BLEU scores for that subset\n",
    "    #    c. Store scores and counts for the bin\n",
    "    # 4. Return DataFrame with BLEU scores for each length bin\n",
    "    \n",
    "    return bleu_df\n",
    "\n",
    "# Analyze BLEU scores by caption length\n",
    "bleu_by_length = analyze_bleu_by_length(greedy_results)\n",
    "print(\"BLEU scores by caption length:\")\n",
    "print(bleu_by_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot BLEU scores by caption length\n",
    "plt.figure(figsize=(14, 8))\n",
    "\n",
    "# Plot BLEU-1 to BLEU-4\n",
    "for i in range(4):\n",
    "    plt.subplot(2, 2, i+1)\n",
    "    metric = f'BLEU-{i+1}'\n",
    "    plt.bar(bleu_by_length.index, bleu_by_length[metric], color=f'C{i}')\n",
    "    plt.xlabel('Caption Length')\n",
    "    plt.ylabel('Score')\n",
    "    plt.title(f'{metric} by Caption Length')\n",
    "    plt.ylim(0, max(bleu_by_length[metric]) * 1.2)  # Add some margin\n",
    "    \n",
    "    # Add count as text below the bars\n",
    "    for j, (idx, count) in enumerate(zip(bleu_by_length.index, bleu_by_length['Count'])):\n",
    "        plt.text(j, 0.01, f'n={count}', ha='center')\n",
    "        plt.text(j, bleu_by_length[metric][j] + 0.01, f'{bleu_by_length[metric][j]:.3f}', ha='center')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Visualize Best and Worst Captions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate per-image BLEU score\n",
    "def calculate_image_bleu(ground_truth, predicted):\n",
    "    \"\"\"Calculate BLEU score for a single image.\"\"\"\n",
    "    bleu = ...\n",
    "    # TODO: Implement BLEU score calculation for a single image\n",
    "    # 1. Tokenize ground truth and predicted captions\n",
    "    # 2. Format ground truth as a list of references\n",
    "    # 3. Calculate and return the BLEU score using sentence_bleu\n",
    "    \n",
    "    return bleu\n",
    "\n",
    "# Calculate BLEU score for each image\n",
    "greedy_results['bleu'] = greedy_results.apply(\n",
    "    lambda row: calculate_image_bleu(row['ground_truth'], row['predicted']),\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# Sort by BLEU score\n",
    "sorted_results = greedy_results.sort_values('bleu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display worst and best captions\n",
    "def display_examples(results_df, image_dir, num_examples=3, best=True):\n",
    "    \"\"\"Display examples of captions.\"\"\"\n",
    "    if best:\n",
    "        examples = results_df.tail(num_examples).iloc[::-1]  # Best examples (highest BLEU)\n",
    "        title = \"Best Captions\"\n",
    "    else:\n",
    "        examples = results_df.head(num_examples)  # Worst examples (lowest BLEU)\n",
    "        title = \"Worst Captions\"\n",
    "    \n",
    "    # Create figure\n",
    "    plt.figure(figsize=(15, 5 * num_examples))\n",
    "    plt.suptitle(title, fontsize=16, y=1.0)\n",
    "    \n",
    "    # Display examples\n",
    "    for i, (_, example) in enumerate(examples.iterrows()):\n",
    "        # Load and display image\n",
    "        img_path = os.path.join(image_dir, 'processed', 'images', example['image_id'])\n",
    "        img = Image.open(img_path)\n",
    "        \n",
    "        plt.subplot(num_examples, 1, i+1)\n",
    "        plt.imshow(img)\n",
    "        plt.title(f\"Image: {example['image_id']}  |  BLEU: {example['bleu']:.4f}\")\n",
    "        plt.axis('off')\n",
    "        \n",
    "        # Add captions as text\n",
    "        caption_text = f\"Ground Truth: {example['ground_truth']}\\n\\n\"\n",
    "        caption_text += f\"Predicted: {example['predicted']}\"\n",
    "        \n",
    "        plt.figtext(0.5, 0.01 + i * (1/num_examples), caption_text, \n",
    "                   ha='center', fontsize=12, bbox={\"facecolor\":\"white\", \"alpha\":0.8, \"pad\":5})\n",
    "    \n",
    "    plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "    plt.show()\n",
    "\n",
    "# Display worst captions\n",
    "display_examples(sorted_results, data_dir, num_examples=3, best=False)\n",
    "\n",
    "# Display best captions\n",
    "display_examples(sorted_results, data_dir, num_examples=3, best=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Analyze Common Words and Patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze common words in predicted captions\n",
    "def analyze_word_frequency(texts):\n",
    "    \"\"\"Analyze word frequency in a list of texts.\"\"\"\n",
    "    word_df = ...\n",
    "    # TODO: Implement word frequency analysis\n",
    "    # 1. Initialize a counter for words\n",
    "    # 2. For each text:\n",
    "    #    a. Tokenize the text\n",
    "    #    b. Update the counter with each word\n",
    "    # 3. Convert the counter to a DataFrame\n",
    "    # 4. Sort by frequency and return the DataFrame\n",
    "    \n",
    "    return word_df\n",
    "\n",
    "# Analyze ground truth and predicted captions\n",
    "gt_words = analyze_word_frequency(greedy_results['ground_truth'])\n",
    "pred_words = analyze_word_frequency(greedy_results['predicted'])\n",
    "\n",
    "# Display top words\n",
    "top_n = 20\n",
    "print(f\"Top {top_n} words in ground truth captions:\")\n",
    "print(gt_words.head(top_n))\n",
    "print(f\"\\nTop {top_n} words in predicted captions:\")\n",
    "print(pred_words.head(top_n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot word frequency comparison\n",
    "def plot_word_comparison(gt_words, pred_words, top_n=15):\n",
    "    \"\"\"Plot comparison of word frequencies.\"\"\"\n",
    "    # Select top words from both sets\n",
    "    all_top_words = set(gt_words['word'].head(top_n)).union(set(pred_words['word'].head(top_n)))\n",
    "    \n",
    "    # Create comparison dataframe\n",
    "    comparison = []\n",
    "    for word in all_top_words:\n",
    "        gt_count = gt_words[gt_words['word'] == word]['count'].values[0] if word in gt_words['word'].values else 0\n",
    "        pred_count = pred_words[pred_words['word'] == word]['count'].values[0] if word in pred_words['word'].values else 0\n",
    "        \n",
    "        comparison.append({\n",
    "            'word': word,\n",
    "            'ground_truth': gt_count,\n",
    "            'predicted': pred_count,\n",
    "            'diff': pred_count - gt_count\n",
    "        })\n",
    "    \n",
    "    # Convert to dataframe and sort by total count\n",
    "    comp_df = pd.DataFrame(comparison)\n",
    "    comp_df['total'] = comp_df['ground_truth'] + comp_df['predicted']\n",
    "    comp_df = comp_df.sort_values('total', ascending=False).head(top_n)\n",
    "    \n",
    "    # Plot\n",
    "    plt.figure(figsize=(14, 7))\n",
    "    \n",
    "    x = np.arange(len(comp_df))\n",
    "    width = 0.35\n",
    "    \n",
    "    plt.bar(x - width/2, comp_df['ground_truth'], width, label='Ground Truth', color='blue')\n",
    "    plt.bar(x + width/2, comp_df['predicted'], width, label='Predicted', color='green')\n",
    "    \n",
    "    plt.xlabel('Word')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.title(f'Top {top_n} Words Comparison')\n",
    "    plt.xticks(x, comp_df['word'], rotation=45)\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Plot words with largest discrepancy\n",
    "    plt.figure(figsize=(14, 7))\n",
    "    \n",
    "    # Sort by absolute difference\n",
    "    diff_df = comp_df.copy()\n",
    "    diff_df['abs_diff'] = diff_df['diff'].abs()\n",
    "    diff_df = diff_df.sort_values('abs_diff', ascending=False).head(top_n)\n",
    "    \n",
    "    # Plot differences\n",
    "    colors = ['red' if d < 0 else 'green' for d in diff_df['diff']]\n",
    "    plt.bar(diff_df['word'], diff_df['diff'], color=colors)\n",
    "    \n",
    "    plt.axhline(y=0, color='black', linestyle='-')\n",
    "    plt.xlabel('Word')\n",
    "    plt.ylabel('Difference (Predicted - Ground Truth)')\n",
    "    plt.title('Words with Largest Frequency Discrepancy')\n",
    "    plt.xticks(rotation=45)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return comp_df\n",
    "\n",
    "# Plot word comparison\n",
    "word_comparison = plot_word_comparison(gt_words, pred_words, top_n=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Create Interactive Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an interactive demo function\n",
    "def interactive_demo(model, vocab, device, data_dir):\n",
    "    \"\"\"Interactive demo for caption generation.\"\"\"\n",
    "    from IPython.display import display, clear_output\n",
    "    import ipywidgets as widgets\n",
    "    \n",
    "    # Load test images\n",
    "    images_dir = os.path.join(data_dir, 'processed', 'images')\n",
    "    image_files = sorted([f for f in os.listdir(images_dir) if f.endswith(('.jpg', '.jpeg', '.png'))])\n",
    "    \n",
    "    # Set up transformations for input images\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(\n",
    "            mean=[0.485, 0.456, 0.406],\n",
    "            std=[0.229, 0.224, 0.225]\n",
    "        )\n",
    "    ])\n",
    "    \n",
    "    # Function to generate caption\n",
    "    def generate_image_caption(image_path, beam_size=1):\n",
    "        \"\"\"Generate caption for an image.\"\"\"\n",
    "        # Load and transform image\n",
    "        img = Image.open(image_path)\n",
    "        img_tensor = transform(img).unsqueeze(0).to(device)\n",
    "        \n",
    "        # Generate caption\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            caption = model.generate_caption(img_tensor, beam_size=beam_size)\n",
    "            caption_text = vocab.decode(caption, join=True, remove_special=True)\n",
    "        \n",
    "        return img, caption_text\n",
    "    \n",
    "    # Function to handle image selection\n",
    "    def on_image_select(change):\n",
    "        # Get selected image\n",
    "        selected_image = change['new']\n",
    "        image_path = os.path.join(images_dir, selected_image)\n",
    "        \n",
    "        # Generate captions with different beam sizes\n",
    "        img, greedy_caption = generate_image_caption(image_path, beam_size=1)\n",
    "        _, beam3_caption = generate_image_caption(image_path, beam_size=3)\n",
    "        _, beam5_caption = generate_image_caption(image_path, beam_size=5)\n",
    "        \n",
    "        # Display image and captions\n",
    "        clear_output(wait=True)\n",
    "        \n",
    "        # Create widgets\n",
    "        display(widgets.HBox([widgets.Label('Select an image:'), image_dropdown]))\n",
    "        \n",
    "        # Display image\n",
    "        plt.figure(figsize=(10, 8))\n",
    "        plt.imshow(img)\n",
    "        plt.title(f\"Image: {selected_image}\")\n",
    "        plt.axis('off')\n",
    "        plt.show()\n",
    "        \n",
    "        # Display captions\n",
    "        print(\"Generated captions:\")\n",
    "        print(f\"Greedy search: {greedy_caption}\")\n",
    "        print(f\"Beam search (k=3): {beam3_caption}\")\n",
    "        print(f\"Beam search (k=5): {beam5_caption}\")\n",
    "    \n",
    "    # Create dropdown widget\n",
    "    image_dropdown = widgets.Dropdown(\n",
    "        options=image_files,\n",
    "        description='Image:',\n",
    "        style={'description_width': 'initial'}\n",
    "    )\n",
    "    \n",
    "    # Register callback\n",
    "    image_dropdown.observe(on_image_select, names='value')\n",
    "    \n",
    "    # Display initial widget\n",
    "    display(widgets.HBox([widgets.Label('Select an image:'), image_dropdown]))\n",
    "\n",
    "# Run interactive demo\n",
    "interactive_demo(model, vocab, device, data_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Summary and Conclusions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print summary statistics\n",
    "print(\"Model Performance Summary\")\n",
    "print(\"=========================\")\n",
    "print(f\"Model type: {config['encoder_model']} + {config['decoder_type']}\")\n",
    "print(f\"Vocabulary size: {len(vocab)} words\")\n",
    "print(f\"Test set size: {len(greedy_results)} images\")\n",
    "print(\"\\nBLEU Scores:\")\n",
    "for i, (greedy, beam) in enumerate(zip(greedy_bleu, beam_bleu)):\n",
    "    print(f\"  BLEU-{i+1}: {greedy:.4f} (greedy search) / {beam:.4f} (beam search)\")\n",
    "\n",
    "print(\"\\nCaption Length:\")\n",
    "print(f\"  Average ground truth: {avg_gt_length:.2f} words\")\n",
    "print(f\"  Average generated: {avg_pred_length:.2f} words\")\n",
    "\n",
    "print(\"\\nObservations:\")\n",
    "print(\"  1. Beam search generally produces slightly better BLEU scores than greedy search\")\n",
    "print(\"  2. BLEU scores decrease as the n-gram size increases (expected behavior)\")\n",
    "print(\"  3. The model tends to generate shorter captions than the ground truth\")\n",
    "print(\"  4. The model performs better on images with common objects and simple scenes\")\n",
    "print(\"  5. The model struggles with complex scenes and unusual activities\")\n",
    "\n",
    "print(\"\\nPossible Improvements:\")\n",
    "print(\"  1. Use a larger dataset (e.g., MSCOCO instead of Flickr8k)\")\n",
    "print(\"  2. Implement attention mechanism to focus on relevant image regions\")\n",
    "print(\"  3. Fine-tune the CNN encoder for better image representations\")\n",
    "print(\"  4. Use a more sophisticated decoder (e.g., Transformer)\")\n",
    "print(\"  5. Train with different loss functions (e.g., CIDEr optimization)\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
