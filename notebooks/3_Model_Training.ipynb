{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Captioning - Model Training\n",
    "\n",
    "In this notebook, we will train the image captioning model using the pre-extracted features from the CNN encoder. We will:\n",
    "\n",
    "1. Set up the data loaders for training and validation\n",
    "2. Build the caption model (combining the encoder and decoder)\n",
    "3. Define the training pipeline with teacher forcing\n",
    "4. Train the model with appropriate hyperparameters\n",
    "5. Monitor the training progress and validation performance\n",
    "6. Save the trained model for later evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import h5py\n",
    "\n",
    "# Add project root to path\n",
    "sys.path.append('..')\n",
    "\n",
    "# Import project modules\n",
    "from models.encoder import EncoderCNN\n",
    "from models.decoder import DecoderRNN\n",
    "from models.caption_model import CaptionModel\n",
    "from utils.vocabulary import Vocabulary\n",
    "from utils.dataset import get_data_loaders\n",
    "from utils.trainer import CaptionTrainer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Configuration Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define configuration settings\n",
    "config = {\n",
    "    # Data settings\n",
    "    'data_dir': '../data/flickr8k',\n",
    "    'batch_size': 32,\n",
    "    'num_workers': 4,\n",
    "    \n",
    "    # Model settings\n",
    "    'encoder_model': 'resnet18',  # Options: 'resnet18', 'resnet50', 'mobilenet_v2'\n",
    "    'embed_size': 256,\n",
    "    'hidden_size': 512,\n",
    "    'num_layers': 1,\n",
    "    'dropout': 0.5,\n",
    "    'decoder_type': 'lstm',  # Options: 'lstm', 'gru'\n",
    "    \n",
    "    # Training settings\n",
    "    'learning_rate': 3e-4,\n",
    "    'num_epochs': 15,  # Increase for better results\n",
    "    'early_stopping_patience': 5,\n",
    "    'save_dir': '../models',\n",
    "    \n",
    "    # Device settings\n",
    "    'device': 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "}\n",
    "\n",
    "# Create save directory if it doesn't exist\n",
    "os.makedirs(config['save_dir'], exist_ok=True)\n",
    "\n",
    "# Print configuration\n",
    "for section, items in {\n",
    "    'Data': ['data_dir', 'batch_size', 'num_workers'],\n",
    "    'Model': ['encoder_model', 'embed_size', 'hidden_size', 'num_layers', 'dropout', 'decoder_type'],\n",
    "    'Training': ['learning_rate', 'num_epochs', 'early_stopping_patience', 'save_dir'],\n",
    "    'Device': ['device']\n",
    "}.items():\n",
    "    print(f\"\\n{section} settings:\")\n",
    "    for item in items:\n",
    "        print(f\"  {item}: {config[item]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get data loaders\n",
    "train_loader, val_loader, test_loader, vocab = get_data_loaders(\n",
    "    data_dir=config['data_dir'],\n",
    "    batch_size=config['batch_size'],\n",
    "    num_workers=config['num_workers']\n",
    ")\n",
    "\n",
    "# Update vocabulary size in config\n",
    "config['vocab_size'] = len(vocab)\n",
    "print(f\"Vocabulary size: {config['vocab_size']}\")\n",
    "\n",
    "# Display data loader information\n",
    "print(f\"Training batches: {len(train_loader)}\")\n",
    "print(f\"Validation batches: {len(val_loader)}\")\n",
    "print(f\"Test batches: {len(test_loader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Create Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create caption model\n",
    "model = CaptionModel(\n",
    "    embed_size=config['embed_size'],\n",
    "    hidden_size=config['hidden_size'],\n",
    "    vocab_size=config['vocab_size'],\n",
    "    num_layers=config['num_layers'],\n",
    "    encoder_model=config['encoder_model'],\n",
    "    decoder_type=config['decoder_type'],\n",
    "    dropout=config['dropout'],\n",
    "    train_encoder=False  # Don't train the encoder (use pre-trained weights)\n",
    ")\n",
    "\n",
    "# Print model architecture\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate model size\n",
    "def count_parameters(model):\n",
    "    \"\"\"Count the number of trainable parameters in a model.\"\"\"\n",
    "    num_of_trainable_parameters = ...\n",
    "    return num_of_trainable_parameters\n",
    "\n",
    "# Calculate and print model size\n",
    "num_params = count_parameters(model)\n",
    "print(f\"Number of trainable parameters: {num_params:,}\")\n",
    "\n",
    "# Calculate encoder and decoder sizes separately\n",
    "encoder_params = count_parameters(model.encoder)\n",
    "decoder_params = count_parameters(model.decoder)\n",
    "print(f\"Encoder parameters: {encoder_params:,} ({encoder_params / num_params * 100:.1f}%)\")\n",
    "print(f\"Decoder parameters: {decoder_params:,} ({decoder_params / num_params * 100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create trainer\n",
    "trainer = CaptionTrainer(\n",
    "    model=model,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    vocab=vocab,\n",
    "    device=config['device'],\n",
    "    learning_rate=config['learning_rate'],\n",
    "    model_save_dir=config['save_dir']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "history = trainer.train(\n",
    "    epochs=config['num_epochs'],\n",
    "    early_stopping_patience=config['early_stopping_patience'],\n",
    "    evaluate_every=1,\n",
    "    generate_every=5  # Generate captions and calculate BLEU every 5 epochs\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Analyze Training Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training history\n",
    "fig, axs = trainer.plot_history()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find best epoch\n",
    "best_epoch = np.argmin(trainer.history['val_loss'])\n",
    "print(f\"Best epoch: {best_epoch + 1}\")\n",
    "print(f\"Best validation loss: {trainer.history['val_loss'][best_epoch]:.4f}\")\n",
    "\n",
    "# If BLEU scores were calculated\n",
    "bleu_epochs = [i for i, bleu in enumerate(trainer.history['val_bleu']) if bleu > 0]\n",
    "if bleu_epochs:\n",
    "    best_bleu_epoch = bleu_epochs[np.argmax([trainer.history['val_bleu'][i] for i in bleu_epochs])]\n",
    "    print(f\"Best BLEU epoch: {best_bleu_epoch + 1}\")\n",
    "    print(f\"Best BLEU score: {trainer.history['val_bleu'][best_bleu_epoch]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Generate Sample Captions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the best model\n",
    "best_model_path = os.path.join(config['save_dir'], 'best_model_loss.pth')\n",
    "trainer.load_checkpoint(best_model_path)\n",
    "model = trainer.model.to(config['device'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate captions for some validation images\n",
    "def generate_caption(image, model, vocab, device):\n",
    "    \"\"\"Generate a caption for an image.\"\"\"\n",
    "    \n",
    "    caption_text = ...\n",
    "    # TODO: Implement the caption generation function\n",
    "    # 1. Set the model to evaluation mode\n",
    "    # 2. Use torch.no_grad() to disable gradient calculation during inference\n",
    "    # 3. Move the image to the device and add a batch dimension if needed\n",
    "    # 4. Generate a caption using the model's generate_caption method\n",
    "    # 5. Decode the caption indices to text using the vocabulary\n",
    "    # 6. Return the caption text\n",
    "    \n",
    "    return caption_text\n",
    "\n",
    "# Get some validation examples\n",
    "num_examples = 5\n",
    "val_examples = []\n",
    "\n",
    "for images, captions, image_ids in val_loader:\n",
    "    if len(val_examples) >= num_examples:\n",
    "        break\n",
    "    \n",
    "    # Generate captions\n",
    "    for i in range(len(images)):\n",
    "        if len(val_examples) >= num_examples:\n",
    "            break\n",
    "        \n",
    "        image = images[i]\n",
    "        true_caption = vocab.decode(captions[i], join=True, remove_special=True)\n",
    "        generated_caption = generate_caption(image, model, vocab, config['device'])\n",
    "        \n",
    "        val_examples.append({\n",
    "            'image': image,\n",
    "            'image_id': image_ids[i],\n",
    "            'true_caption': true_caption,\n",
    "            'generated_caption': generated_caption\n",
    "        })\n",
    "\n",
    "# Display examples\n",
    "plt.figure(figsize=(15, 5 * num_examples))\n",
    "\n",
    "for i, example in enumerate(val_examples):\n",
    "    plt.subplot(num_examples, 1, i + 1)\n",
    "    \n",
    "    # Convert tensor to image\n",
    "    img = example['image'].permute(1, 2, 0).numpy()\n",
    "    img = (img * [0.229, 0.224, 0.225]) + [0.485, 0.456, 0.406]  # Denormalize\n",
    "    img = np.clip(img, 0, 1)\n",
    "    \n",
    "    plt.imshow(img)\n",
    "    plt.title(f\"Image: {example['image_id']}\")\n",
    "    plt.axis('off')\n",
    "    plt.figtext(0.5, 0.01 + i * (1/num_examples), f\"True caption: {example['true_caption']}\", \n",
    "                ha='center', fontsize=12, bbox={\"facecolor\":\"white\", \"alpha\":0.5, \"pad\":5})\n",
    "    plt.figtext(0.5, 0.05 + i * (1/num_examples), f\"Generated caption: {example['generated_caption']}\", \n",
    "                ha='center', fontsize=12, bbox={\"facecolor\":\"lightgreen\", \"alpha\":0.5, \"pad\":5})\n",
    "\n",
    "plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Compare Greedy and Beam Search Decoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare greedy decoding vs. beam search\n",
    "def compare_decoding_methods(image, model, vocab, device, beam_sizes=[1, 3, 5]):\n",
    "    \"\"\"Compare different beam search sizes for caption generation.\"\"\"\n",
    "    results = ...\n",
    "    # TODO: Implement a function to compare different beam search settings\n",
    "    # 1. Set the model to evaluation mode\n",
    "    # 2. Initialize a dictionary to store results\n",
    "    # 3. For each beam size:\n",
    "    #    a. Generate a caption using that beam size\n",
    "    #    b. Decode the caption indices to text\n",
    "    #    c. Store the result in the dictionary\n",
    "    # 4. Return the dictionary of results\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Select a random example from val_examples\n",
    "import random\n",
    "example = random.choice(val_examples)\n",
    "\n",
    "# Compare decoding methods\n",
    "beam_results = compare_decoding_methods(\n",
    "    example['image'], \n",
    "    model, \n",
    "    vocab, \n",
    "    config['device'], \n",
    "    beam_sizes=[1, 3, 5]\n",
    ")\n",
    "\n",
    "# Display the image with different captions\n",
    "plt.figure(figsize=(10, 10))\n",
    "\n",
    "# Display image\n",
    "img = example['image'].permute(1, 2, 0).numpy()\n",
    "img = (img * [0.229, 0.224, 0.225]) + [0.485, 0.456, 0.406]  # Denormalize\n",
    "img = np.clip(img, 0, 1)\n",
    "plt.imshow(img)\n",
    "plt.axis('off')\n",
    "plt.title(f\"Image: {example['image_id']}\")\n",
    "\n",
    "# Add captions\n",
    "captions = [\n",
    "    f\"True caption: {example['true_caption']}\",\n",
    "    f\"Greedy search: {beam_results['beam_1']}\",\n",
    "    f\"Beam search (k=3): {beam_results['beam_3']}\",\n",
    "    f\"Beam search (k=5): {beam_results['beam_5']}\"\n",
    "]\n",
    "\n",
    "# Display captions below the image\n",
    "plt.figtext(0.5, 0.01, '\\n'.join(captions), ha='center', fontsize=12, \n",
    "            bbox={\"facecolor\":\"white\", \"alpha\":0.8, \"pad\":5})\n",
    "\n",
    "plt.tight_layout(rect=[0, 0.2, 1, 0.95])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Save Final Model and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save configuration\n",
    "import json\n",
    "config_path = os.path.join(config['save_dir'], 'config.json')\n",
    "\n",
    "# Convert non-serializable values to strings\n",
    "serializable_config = {k: str(v) if not isinstance(v, (int, float, str, bool)) else v \n",
    "                      for k, v in config.items()}\n",
    "\n",
    "with open(config_path, 'w') as f:\n",
    "    json.dump(serializable_config, f, indent=4)\n",
    "\n",
    "print(f\"Configuration saved to {config_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save vocabulary separately for easy access\n",
    "import pickle\n",
    "vocab_path = os.path.join(config['save_dir'], 'vocabulary.pkl')\n",
    "with open(vocab_path, 'wb') as f:\n",
    "    pickle.dump(vocab, f)\n",
    "\n",
    "print(f\"Vocabulary saved to {vocab_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this notebook, we have successfully:\n",
    "\n",
    "1. Set up the data loaders for training and validation\n",
    "2. Built the caption model by combining the encoder and decoder\n",
    "3. Trained the model with teacher forcing\n",
    "4. Monitored the training progress and validation performance\n",
    "5. Generated captions for sample images\n",
    "6. Compared different decoding strategies (greedy vs. beam search)\n",
    "7. Saved the model, configuration, and vocabulary for later use\n",
    "\n",
    "In the next notebook, we will perform a comprehensive evaluation of the model on the test set."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
